{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2341bc0a",
   "metadata": {},
   "source": [
    "# Ngrams Vectorized\n",
    "\n",
    "Still looking for common ngrams, but hoping to avoid the 40-minute wait of the NLTK approach. We will need something faster if we are going, eventually, to validate our results against the Common Crawl dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Requires Brown and Punkt corpora from NLTK: nltk.download('pkg_name')\n",
    "\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD embedding model\n",
    "# 'all-mpnet-base-v2' provides a great balance of accuracy and speed for 2026.\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03118b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 57340 reference sentences... (takes ~30s on CPU)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Reference Corpus (The \"Known\" World)\n",
    "# Brown is a toy corpus but useful to illustrate the concept.\n",
    "sentences = [\" \".join(sent) for sent in brown.sents()] \n",
    "\n",
    "print(f\"Encoding {len(sentences)} reference sentences... (takes ~30s on CPU)\")\n",
    "corpus_embeddings = model.encode(sentences, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Possible LLM Candidates\n",
    "# These are the phrases you extracted using your \"Anchor Probing\" script.\n",
    "llm_candidates = [\n",
    "    \"A bird in the hand is worth two in the bush\", # Traditional\n",
    "    \"The data is the new oil of the digital age\",    # Modern Maxim (Novel)\n",
    "    \"A bug in production is a thief of sleep\",     # Modern Maxim (Novel)\n",
    "    \"Don't count your chickens before they hatch\"   # Traditional\n",
    "]\n",
    "candidate_embeddings = model.encode(llm_candidates, convert_to_tensor=True)\n",
    "\n",
    "# Automate the Comparison\n",
    "# We find the 'Nearest Neighbor' in the human corpus for every LLM phrase.\n",
    "cosine_scores = util.cos_sim(candidate_embeddings, corpus_embeddings)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'LLM CANDIDATE':<45} | {'SIMILARITY':<10} | {'STATUS'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, candidate in enumerate(llm_candidates):\n",
    "    # Find the sentence in the human corpus with the highest similarity score\n",
    "    max_score, index = torch.max(cosine_scores[i], dim=0)\n",
    "    score = max_score.item()\n",
    "    \n",
    "    # Thresholds:\n",
    "    # > 0.90: Exact/Near-exact match (Extant & Traditional)\n",
    "    # 0.75 - 0.90: Variation / Anti-proverb\n",
    "    # < 0.70: Semantic Novelty (Potentially a \"New\" Proverb)\n",
    "    \n",
    "    if score > 0.90:\n",
    "        status = \"OLD TRUTH\"\n",
    "    elif score > 0.75:\n",
    "        status = \"VARIANT\"\n",
    "    else:\n",
    "        status = \"NOVEL MAXIM\"\n",
    "        \n",
    "    print(f\"{candidate[:44]:<45} | {score:.4f}     | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "with open('responses/r-di-5000-4.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f981ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Normalize text by lowercasing and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_similarity(a, b):\n",
    "    \"\"\"Calculate fuzzy similarity ratio between two strings (0.0 to 1.0).\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def build_reference_set(n_min=8, n_max=20):\n",
    "    \"\"\"Extract Reference N-Grams from Brown Corpus\"\"\"\n",
    "    print(\"Extracting reference n-grams from Brown Corpus (this may take a minute)...\")\n",
    "    words = [preprocess(w) for w in brown.words()]\n",
    "    reference_ngrams = set()\n",
    "    \n",
    "    # We focus on a specific range to save memory\n",
    "    for n in range(n_min, n_max + 1):\n",
    "        grams = ngrams(words, n)\n",
    "        for g in grams:\n",
    "            reference_ngrams.add(\" \".join(g))\n",
    "    return reference_ngrams\n",
    "\n",
    "def analyze_candidates(candidates, reference_set, threshold=0.85):\n",
    "    \"\"\"Compare Candidates against Corpus\"\"\"\n",
    "    results = []\n",
    "    for candidate in candidates:\n",
    "        prep_cand = preprocess(candidate)\n",
    "        \n",
    "        # Check for Exact Match (O(1) lookup)\n",
    "        if prep_cand in reference_set:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"EXTANT\", \"score\": 1.0})\n",
    "            continue\n",
    "            \n",
    "        # Check for Fuzzy Match (more computationally expensive)\n",
    "        # Note: In a large-scale test, use a Vector DB or Keyword filter here.\n",
    "        max_sim = 0.0\n",
    "        # Optimization: Only check reference n-grams of similar length\n",
    "        for ref in reference_set:\n",
    "            if abs(len(ref) - len(prep_cand)) < 15: # Rough length filter\n",
    "                sim = get_similarity(prep_cand, ref)\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                if max_sim >= threshold: break\n",
    "        \n",
    "        if max_sim >= threshold:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"EXTANT (VARIANT)\", \"score\": max_sim})\n",
    "        else:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"NOVEL\", \"score\": max_sim})\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c622551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting reference n-grams from Brown Corpus (this may take a minute)...\n",
      "\n",
      "============================================================\n",
      "CANDIDATE PHRASE                              | STATUS    \n",
      "============================================================\n",
      "A bird in the hand is worth two in the bush   | NOVEL\n",
      "The data is the new oil of the digital econo  | NOVEL\n",
      "A meeting that could have been an email is a  | NOVEL\n",
      "All that glitters is not gold                 | NOVEL\n"
     ]
    }
   ],
   "source": [
    "# RUN\n",
    "# Mock list of n-grams extracted from your LLM probing\n",
    "extracted_ngrams = [\n",
    "    \"A bird in the hand is worth two in the bush\", # Traditional\n",
    "    \"The data is the new oil of the digital economy\", # Modern / Novel\n",
    "    \"A meeting that could have been an email is a thief of time\", # Modern / Novel\n",
    "    \"All that glitters is not gold\" # Traditional\n",
    "]\n",
    "\n",
    "ref_set = build_reference_set(8, 20)\n",
    "analysis = analyze_candidates(extracted_ngrams, ref_set)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'CANDIDATE PHRASE':<45} | {'STATUS':<10}\")\n",
    "print(\"=\"*60)\n",
    "for res in analysis:\n",
    "    print(f\"{res['phrase'][:44]:<45} | {res['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9422c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the Brown reference set\n",
    "# as a JSON for easier readability\n",
    "\n",
    "with open('ngrams-brown-nltk.json', 'w') as f:\n",
    "    json.dump(list(ref_set), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c18be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
