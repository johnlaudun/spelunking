{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abc7877",
   "metadata": {},
   "source": [
    "# 5 - Ground Truth Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86a26c",
   "metadata": {},
   "source": [
    "I'm skipping past building a semantic vector space based on Reddit for now, and going straight to ground truth verification using search results. \n",
    "\n",
    "* [pistocop/subreddit-comments-dl: Download subreddit comments](https://github.com/pistocop/subreddit-comments-dl)\n",
    "* [serpapi documentation on PyPI](https://pypi.org/project/serpapi/)\n",
    "* [My SerpApi Dashboard](https://serpapi.com/searches)\n",
    "* [Google Gemini](https://gemini.google.com/app/aa2876187db79f27) notes.\n",
    "\n",
    "To run **all-MiniLM-L6-v2** locally: [StackOverflow](https://stackoverflow.com/questions/65419499/download-pre-trained-sentence-transformers-model-locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS, KEYS, MODELS\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load API key\n",
    "keys = json.load(open(\"../apikeys.json\"))\n",
    "SERP_API_KEY = keys[\"SerpApi\"][\"key\"]\n",
    "\n",
    "# Load model (all-MiniLM-L6-v2 is fast and accurate)\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('../models/BERT-all-mpnet-base-v2')\n",
    "\n",
    "# SAVE MODEL (for offline use)\n",
    "# model.save('../models/BERT-all-mpnet-base-v2')\n",
    "# THEN TO LOAD IT AGAIN:\n",
    "# model = SentenceTransformer(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b286c5",
   "metadata": {},
   "source": [
    "## Loading Results & Getting Rid of Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff7cd1",
   "metadata": {},
   "source": [
    "The first thing to do is to re-use some code to load the saved results of queries and then to find the repetitions. After that, we can work on merging similar proverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334ed272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Proverbs: 1059\n",
      "Total Repetitions: 3941\n"
     ]
    }
   ],
   "source": [
    "# LOAD FILE\n",
    "with open('responses/di-5000-4.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# EXTRACT JUST THE TEXT FROM RESULTS\n",
    "texts = [entry['text'] for entry in data]\n",
    "\n",
    "# COUNT OCCURRENCES\n",
    "counts = Counter(texts)\n",
    "\n",
    "# FIND PROVERBS THAT APPEARED MORE THAN ONCE\n",
    "duplicates = {text: count for text, count in counts.items() if count > 1}\n",
    "print(f\"Unique Proverbs: {len(counts)}\")\n",
    "print(f\"Total Repetitions: {sum(duplicates.values()) - len(duplicates)}\")\n",
    "\n",
    "# TO SEE REPETITIONS\n",
    "# print(\"\\nMost Frequent Repetitions:\")\n",
    "# for text, count in sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[30:40]:\n",
    "#     print(f\"[{count}x] {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517190af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER WAY TO DEDUPE \n",
    "# (no count is generated)\n",
    "# deduped = list(set(texts))\n",
    "\n",
    "# SORT BY LENGTH SO THE SHORTEST VERSION IS OUR \"ANCHOR\"\n",
    "deduped = list(duplicates.keys()) # If first method is used\n",
    "data = sorted(deduped, key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e35b17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4026f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Count: 246 | Final Count: 37 ---\n",
      "‚úì Don't read the comments.\n",
      "‚úì The internet is forever so be careful what you post.\n",
      "‚úì If you're not paying for the product, you are the product.\n",
      "‚úì The internet is forever and nothing is ever truly deleted.\n",
      "‚úì If you lurk on the internet long enough, eventually you see yourself in a post.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone get banned.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone mention their ex.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone mention their grandma.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a variation of the same 10 arguments.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone compare themselves to Hitler.\n",
      "‚úì If you lurk on the internet long enough, eventually you'll see a variation of your own personality disorder.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell a story that is 100% them.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user throw shade at their own mother.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user to touch grass.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user defend their terrible life choices.\n",
      "‚úì If you lurk long enough on any online community, eventually you'll see someone's offline drama play out in real time.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone compare their life to a highlight reel.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user that they have cancer.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user try to relive a past argument they lost.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see yourself in a heated argument with a reflection.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user defend their right to be mildly annoying.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user argue that their username was taken first.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user to take their problem to Twitter.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone mention that they're not a fan of pineapple on pizza.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user to just log off if they don't like it.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user to get a girlfriend or get outside more.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone say that the internet is a great tool for making enemies.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user complain about how the platform has changed since they first joined.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a variation of the same fight that was already old when you first showed up.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user tell another user that the internet is a reflection of the real world.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone mention their cousin's brother-in-law who knows a guy who got rich off\n",
      "‚úì If you lurk long enough on any online community, eventually you'll see someone mention that they're not a doctor but they've got a degree from Google.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user say that they were having a great day until they read the comments section\n",
      "‚úì If you lurk long enough on any online community, eventually you'll see someone mention that they're not a lawyer but they'll give you legal advice anyway.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see yourself in a heated argument with someone who is probably you in an alternate timeline.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see someone mention that they're not a therapist but they'll try to offer some advice anyway.\n",
      "‚úì If you lurk long enough on any online community, you'll eventually see a user post something about how they never meant to spend three hours scrolling through their\n"
     ]
    }
   ],
   "source": [
    "unique_proverbs = []\n",
    "threshold = 0.60  # Aggressive grouping\n",
    "\n",
    "for current in data:\n",
    "    if not unique_proverbs:\n",
    "        unique_proverbs.append(current)\n",
    "        continue\n",
    "    \n",
    "    # Compare current sentence against our accepted unique list\n",
    "    current_emb = model.encode(current, convert_to_tensor=True)\n",
    "    unique_embs = model.encode(unique_proverbs, convert_to_tensor=True)\n",
    "    \n",
    "    scores = util.cos_sim(current_emb, unique_embs)[0]\n",
    "    \n",
    "    # If it's not similar to anything we already have, add it\n",
    "    if max(scores) < threshold:\n",
    "        unique_proverbs.append(current)\n",
    "\n",
    "# Display results\n",
    "print(f\"--- Original Count: {len(data)} | Final Count: {len(unique_proverbs)} ---\")\n",
    "for p in unique_proverbs:\n",
    "    print(f\"‚úì {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c26991",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLDER METHOD BELOW\n",
    "\n",
    "# Process the proverbs to filter out similar meanings\n",
    "\n",
    "uniques = []\n",
    "\n",
    "for proverb in proverbs:\n",
    "    if not uniques:\n",
    "        uniques.append(proverb)\n",
    "        continue\n",
    "    \n",
    "    # Encode current sentence and existing unique ones\n",
    "    current_embedding = model.encode(proverb)\n",
    "    unique_embeddings = model.encode(uniques)\n",
    "    \n",
    "    # Calculate similarities between current sentence and all saved unique ones\n",
    "    cosine_scores = util.cos_sim(current_embedding, unique_embeddings)[0]\n",
    "    \n",
    "    # If the highest similarity score is below our threshold, it's a \"new\" proverb\n",
    "    # We'll use 0.75 as a standard threshold for \"same meaning\"\n",
    "    threshold = 0.50\n",
    "    if max(cosine_scores) < threshold:\n",
    "        uniques.append(proverb)\n",
    "\n",
    "# 4. Output the results\n",
    "\n",
    "for proverb in uniques:\n",
    "    print(f\" - {proverb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfc7e9",
   "metadata": {},
   "source": [
    "## Search / Validate\n",
    "\n",
    "Having whittled down the responses from the LLM to a manageable number of unique proverbs, we can now use SerpApi to search for each proverb and see if there are any results. If there are results, we can assume that the proverb is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_external_existence(phrase):\n",
    "    \"\"\"\n",
    "    Checks the web for the phrase and looks for 'canonical' markers.\n",
    "    \"\"\"\n",
    "    # 1. Total Hit Count Check (Exact Phrase)\n",
    "    params = {\n",
    "        \"q\": f'\"{phrase}\"',  # Quoted for exact match\n",
    "        \"engine\": \"google\",\n",
    "        \"api_key\": SERP_API_KEY\n",
    "    }\n",
    "    \n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    # Extract total results (Google hit count)\n",
    "    total_results = results.get(\"search_information\", {}).get(\"total_results\", 0)\n",
    "    \n",
    "    # 2. Targeted Site Check (Dictionary & Folklore sites)\n",
    "    # We check if the phrase appears on known authority sites\n",
    "    authority_sites = [\"oxfordreference.com\", \"phrases.org.uk\", \"theidioms.com\"]\n",
    "    site_query = f'\"{phrase}\" site:' + \" OR site:\".join(authority_sites)\n",
    "    \n",
    "    site_params = {**params, \"q\": site_query}\n",
    "    site_search = GoogleSearch(site_params)\n",
    "    site_results = site_search.get_dict()\n",
    "    \n",
    "    authority_count = site_results.get(\"search_information\", {}).get(\"total_results\", 0)\n",
    "    \n",
    "    # 3. Novelty Logic\n",
    "    # High LLM Stability + Low Search Hits = A Discovery\n",
    "    if total_results < 1000 and authority_count == 0:\n",
    "        return {\n",
    "            \"verdict\": \"üåü NOVEL MAXIM\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": \"High consensus in AI, but virtually zero footprint in human dictionaries.\"\n",
    "        }\n",
    "    elif authority_count > 0:\n",
    "        return {\n",
    "            \"verdict\": \"üìö DOCUMENTED PROVERB\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": f\"Found on {authority_count} authority websites.\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"verdict\": \"üåê COMMON IDIOM\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": \"Frequently used online but not officially documented as a proverb.\"\n",
    "        }\n",
    "\n",
    "# Example Test\n",
    "# discovery = verify_external_existence(\"The data is the new oil of the digital soul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = verify_external_existence(\"The internet is forever and nothing is ever really deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = display\n",
    "d2 = verify_external_existence(\"If you lurk long enough on any online community, you'll eventually see yourself in a post.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624b536",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "In the top 10 results from the run of 5000 queries on Llama-4[^1], there were really three proverbs. The most repeated one occupied two of the top 10 slots:\n",
    "\n",
    "[1139x] If you're not paying for the product, you are the product.  \n",
    "[219x] If you're not paying for the product, then you are the product.\n",
    "\n",
    "So the question I have to determine is if there is a way to recognize two or more proverbs that are semantically the same but lexically different. By hand, I chose the shortest version of the next two proverbs in the top 10 -- and the three proverbs accounted for all of the top 10 results:\n",
    "\n",
    "\n",
    "[62x][253x][558x][73x][70x][66x] The internet is forever and nothing is ever really deleted.  \n",
    "[56x][54x] If you lurk long enough on any online community, you'll eventually see yourself in a post.\n",
    "\n",
    "So find all the proverbs that are the same, choose the shortest version, and search on that.\n",
    "\n",
    "[^1]: The complete descriptor is Llama-4-Maverick-17B-128E-Instruct-FP8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
