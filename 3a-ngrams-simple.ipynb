{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2341bc0a",
   "metadata": {},
   "source": [
    "# Ngrams\n",
    "\n",
    "In the generating and inspection notebooks, we simply count the number of times a given proverb is repeated. One of the things we notice is that there is a lot of repetition of certain proverbs, usually with fairly small variations that do not change the meaning of the proverb.\n",
    "\n",
    "One step in looking for the commonalities between proverbs is to look for ngrams. An n-gram is a contiguous sequence of 'n' items from a given sample of text or speech. Ngrams are widely used in natural language processing for tasks like text prediction, language modeling, and machine translation. \n",
    "\n",
    "Here we are looking to see if we can map the underlying structure of the proverbs by looking for common ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4384136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Requires Brown and Punkt corpora from NLTK: nltk.download('pkg_name')\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Read file into one big text:\n",
    "# Read file into one big text:\n",
    "with open('responses/r-di-5000-4.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Normalize text by lowercasing and removing punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_similarity(a, b):\n",
    "    \"\"\"Calculate fuzzy similarity ratio between two strings (0.0 to 1.0).\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def build_reference_set(n_min=8, n_max=20):\n",
    "    \"\"\"Extract Reference N-Grams from Brown Corpus\"\"\"\n",
    "    print(\"Extracting reference n-grams from Brown Corpus (this may take a minute)...\")\n",
    "    words = [preprocess(w) for w in brown.words()]\n",
    "    reference_ngrams = set()\n",
    "    \n",
    "    # We focus on a specific range to save memory\n",
    "    for n in range(n_min, n_max + 1):\n",
    "        grams = ngrams(words, n)\n",
    "        for g in grams:\n",
    "            reference_ngrams.add(\" \".join(g))\n",
    "    return reference_ngrams\n",
    "\n",
    "def analyze_candidates(candidates, reference_set, threshold=0.85):\n",
    "    \"\"\"Compare Candidates against Corpus\"\"\"\n",
    "    results = []\n",
    "    for candidate in candidates:\n",
    "        prep_cand = preprocess(candidate)\n",
    "        \n",
    "        # Check for Exact Match (O(1) lookup)\n",
    "        if prep_cand in reference_set:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"EXTANT\", \"score\": 1.0})\n",
    "            continue\n",
    "            \n",
    "        # Check for Fuzzy Match (more computationally expensive)\n",
    "        # Note: In a large-scale test, use a Vector DB or Keyword filter here.\n",
    "        max_sim = 0.0\n",
    "        # Optimization: Only check reference n-grams of similar length\n",
    "        for ref in reference_set:\n",
    "            if abs(len(ref) - len(prep_cand)) < 15: # Rough length filter\n",
    "                sim = get_similarity(prep_cand, ref)\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                if max_sim >= threshold: break\n",
    "        \n",
    "        if max_sim >= threshold:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"EXTANT (VARIANT)\", \"score\": max_sim})\n",
    "        else:\n",
    "            results.append({\"phrase\": candidate, \"status\": \"NOVEL\", \"score\": max_sim})\n",
    "            \n",
    "    return results\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Mock list of n-grams extracted from your LLM probing\n",
    "extracted_ngrams = [\n",
    "    \"A bird in the hand is worth two in the bush\", # Traditional\n",
    "    \"The data is the new oil of the digital economy\", # Modern / Novel\n",
    "    \"A meeting that could have been an email is a thief of time\", # Modern / Novel\n",
    "    \"All that glitters is not gold\" # Traditional\n",
    "]\n",
    "\n",
    "ref_set = build_reference_set(8, 20)\n",
    "analysis = analyze_candidates(extracted_ngrams, ref_set)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'CANDIDATE PHRASE':<45} | {'STATUS':<10}\")\n",
    "print(\"=\"*60)\n",
    "for res in analysis:\n",
    "    print(f\"{res['phrase'][:44]:<45} | {res['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdabd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens after preprocessing: 4986\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes text and converts to lowercase, removing punctuation.\"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "# Get the tokenized list from the corpus\n",
    "word_tokens = preprocess_text(one_big_string)\n",
    "print(f\"Total number of tokens after preprocessing: {len(word_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25915bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ngrams_in_range(tokens, min_n, max_n):\n",
    "    \"\"\"\n",
    "    Generates a list of all n-grams for n within the specified range.\n",
    "    \"\"\"\n",
    "    all_ngrams = []\n",
    "    # Loop from min_n up to and including max_n\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        # The ngrams function yields tuples of n tokens\n",
    "        n_gram_generator = ngrams(tokens, n)\n",
    "        # Convert the generator results to a list and extend the master list\n",
    "        all_ngrams.extend(list(n_gram_generator))\n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "413687c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of a generated 8-gram (first one):\n",
      "do compare your to someone else highlight reel\n"
     ]
    }
   ],
   "source": [
    "# Arbitrary numbers until they are not:\n",
    "MIN_N = 8\n",
    "MAX_N = 20\n",
    "\n",
    "# Get all n-grams\n",
    "long_ngrams = get_all_ngrams_in_range(word_tokens, MIN_N, MAX_N)\n",
    "\n",
    "# The n-grams are returned as tuples of tokens, e.g., ('the', 'complexity', 'of', 'modern', 'life', 'often', 'masks', 'the')\n",
    "# We can join them to view them as phrases:\n",
    "print(f\"\\nExample of a generated {MIN_N}-gram (first one):\")\n",
    "print(' '.join(long_ngrams[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1bbae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Most Frequent N-grams (8 to 20 tokens) ---\n",
      "[8-gram, Count: 15]: \"always believe that something wonderful is about to\"\n",
      "[8-gram, Count: 15]: \"believe that something wonderful is about to happen\"\n",
      "[9-gram, Count: 15]: \"always believe that something wonderful is about to happen\"\n",
      "[8-gram, Count: 14]: \"keep your face always toward the sunshine and\"\n",
      "[8-gram, Count: 14]: \"your face always toward the sunshine and shadows\"\n",
      "[8-gram, Count: 14]: \"face always toward the sunshine and shadows will\"\n",
      "[8-gram, Count: 14]: \"always toward the sunshine and shadows will fall\"\n",
      "[8-gram, Count: 14]: \"toward the sunshine and shadows will fall behind\"\n",
      "[8-gram, Count: 14]: \"the sunshine and shadows will fall behind you\"\n",
      "[8-gram, Count: 14]: \"happiness is not the absence of problems it\"\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each unique n-gram\n",
    "ngram_counts = Counter(long_ngrams)\n",
    "\n",
    "# Define how many top results you want to see\n",
    "TOP_K = 10 \n",
    "\n",
    "# Get the top K most common n-grams\n",
    "most_common_ngrams = ngram_counts.most_common(TOP_K)\n",
    "\n",
    "print(f\"\\n--- Top {TOP_K} Most Frequent N-grams ({MIN_N} to {MAX_N} tokens) ---\")\n",
    "for n_gram_tuple, count in most_common_ngrams:\n",
    "    # Join the tuple tokens into a single string phrase\n",
    "    phrase = ' '.join(n_gram_tuple)\n",
    "    n_length = len(n_gram_tuple)\n",
    "    print(f\"[{n_length}-gram, Count: {count}]: \\\"{phrase}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
